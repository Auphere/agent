"""API routes for real-time streaming responses with agent reasoning."""

from __future__ import annotations

import asyncio
import json
from time import perf_counter
from typing import AsyncGenerator
from uuid import UUID, uuid4

from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse

from api.dependencies import (
    get_chat_repo,
    get_context_validator,
    get_conversation_repo,
    get_intent_classifier,
    get_llm_router,
    get_memory_manager,
    get_translator_instance,
)
from api.models import QueryRequest
from src.agents.memory import MemoryManager
from src.classifiers.intent_classifier import IntentClassifier
from src.config import constants
from src.database import ChatRepository, ConversationRepository
from src.i18n import Translator
from src.routers.llm_router import LLMRouter
from src.utils.logger import get_logger
from src.utils.title_generator import generate_chat_title
from src.validators.context_validator import ContextValidator
from src.validators.schemas import ContextValidationError

router = APIRouter(prefix="/agent", tags=["agent-streaming"])
logger = get_logger("streaming_routes")


async def stream_agent_response(
    request: QueryRequest,
    validator: ContextValidator,
    classifier: IntentClassifier,
    llm_router: LLMRouter,
    conversation_repo: ConversationRepository,
    chat_repo: ChatRepository,
    memory_manager: MemoryManager,
    translator: Translator,
) -> AsyncGenerator[str, None]:
    """
    Stream agent response with real-time reasoning steps.
    
    Yields SSE events:
    - status: Processing updates ("thinking", "using_tools", etc.)
    - thought: Agent's reasoning (ReAct THINK step)
    - action: Tool being used (ReAct ACT step)
    - observation: Tool result (ReAct OBSERVE step)
    - token: Response text chunks
    - end: Final complete response
    - error: Error messages
    """
    
    query_id = str(uuid4())
    session_uuid = UUID(request.session_id) if request.session_id else uuid4()
    user_id = request.user_id  # Now accepts string (Auth0 IDs)
    
    try:
        start_time = perf_counter()
        
        # Step 1: Status update - Starting
        yield f"event: status\ndata: {json.dumps({'content': 'üîç Analizando tu consulta...'})}\n\n"
        await asyncio.sleep(0.1)
        
        # Get or create chat
        chat = await chat_repo.get_chat_by_session_id(session_uuid)
        is_new_chat = chat is None
        
        # Step 2: Validate context
        try:
            validated_context = await validator.build_context(request)
        except ContextValidationError as exc:
            error_data = {"content": f"Error de validaci√≥n: {exc.message}"}
            yield f"event: error\ndata: {json.dumps(error_data)}\n\n"
            return
        
        # Step 3: Load memory (using NEW ConversationBuffer system)
        yield f"event: status\ndata: {json.dumps({'content': 'üß† Cargando contexto de conversaci√≥n...'})}\n\n"
        await asyncio.sleep(0.1)
        
        memory_context = await memory_manager.build_agent_context(
            user_id=user_id,
            session_id=session_uuid,
            current_query=request.query,
            include_history=True,
            include_patterns=False,
            current_language=request.language or "es",
        )
        
        # Step 4: Intent classification
        yield f"event: status\ndata: {json.dumps({'content': 'üéØ Clasificando intenci√≥n...'})}\n\n"
        await asyncio.sleep(0.1)
        
        chat_mode = validated_context.metadata.get("chat_mode", "explore")
        
        # If user explicitly selected "plan" mode, force PLAN intent
        if chat_mode == "plan":
            from src.classifiers.models import IntentResult, IntentType
            intent_result = IntentResult(
                intention=IntentType.PLAN,
                confidence=1.0,
                complexity="high",
                reasoning="Usuario seleccion√≥ modo Plan expl√≠citamente"
            )
            
            thought_data = {"content": f"üí≠ **Pensamiento**: {intent_result.reasoning}"}
            yield f"event: thought\ndata: {json.dumps(thought_data)}\n\n"
        else:
            # In explore mode, pass chat_mode to classifier to prevent wrong PLAN classification
            intent_result = await classifier.classify(request.query, validated_context, chat_mode=chat_mode)
            
            thought_data = {
                "content": f"üí≠ **Detect√©**: {intent_result.intention.value} (confianza: {intent_result.confidence:.0%})"
            }
            yield f"event: thought\ndata: {json.dumps(thought_data)}\n\n"
        
        await asyncio.sleep(0.1)
        
        # Step 5: Model routing
        selected_model = llm_router.route(intent_result)
        
        yield f"event: status\ndata: {json.dumps({'content': f'ü§ñ Usando modelo: {selected_model.name}'})}\n\n"
        await asyncio.sleep(0.1)
        
        # Step 6: Execute agent
        from src.agents.supervisor_agent import SupervisorAgent
        
        supervisor = SupervisorAgent()
        
        yield f"event: status\ndata: {json.dumps({'content': '‚ö° Ejecutando agente especializado...'})}\n\n"
        await asyncio.sleep(0.1)
        
        # Show agent thinking
        thought_data = {
            "content": f"üí≠ **Razonando**: Voy a buscar {request.query[:50]}... en la base de datos de lugares"
        }
        yield f"event: thought\ndata: {json.dumps(thought_data)}\n\n"
        await asyncio.sleep(0.2)
        
        # Execute agent (this is the actual work)
        # ‚úÖ FIX: Merge memory_context directly into context (not nested)
        agent_context = {
            "user_id": str(user_id),
            "session_id": str(session_uuid),
            "validated_context": validated_context.dict(),
            **memory_context,  # ‚Üê Unpack memory_context directly
        }
        
        agent_result = await supervisor.run(
            query=request.query,
            intent=intent_result.intention,
            language=request.language or "es",
            context=agent_context,
        )
        
        # Show actions taken
        if agent_result.get("places"):
            action_data = {
                "content": f"üîß **Acci√≥n**: Encontr√© {len(agent_result['places'])} lugares relevantes"
            }
            yield f"event: action\ndata: {json.dumps(action_data)}\n\n"
            await asyncio.sleep(0.2)
        
        # Stream response text word by word
        response_text = agent_result.get("response_text", "")
        
        if response_text:
            yield f"event: status\ndata: {json.dumps({'content': '‚úçÔ∏è Generando respuesta...'})}\n\n"
            await asyncio.sleep(0.1)
            
            words = response_text.split()
            chunk_size = 3  # words per chunk
            
            for i in range(0, len(words), chunk_size):
                chunk = " ".join(words[i:i+chunk_size])
                if i + chunk_size < len(words):
                    chunk += " "
                
                token_data = {"content": chunk}
                yield f"event: token\ndata: {json.dumps(token_data)}\n\n"
                
                await asyncio.sleep(0.05)
        
        # Step 7: Persistence
        yield f"event: status\ndata: {json.dumps({'content': 'üíæ Guardando conversaci√≥n...'})}\n\n"
        await asyncio.sleep(0.1)
        
        # Create/update chat
        if is_new_chat:
            title = await generate_chat_title(request.query, agent_result.get("response_text", ""))
            chat = await chat_repo.create_chat(
                user_id=user_id,
                session_id=session_uuid,
                title=title,
                mode=chat_mode,
            )
            # Commit immediately so frontend can fetch the chat
            await chat_repo.session.commit()
        
        # Save conversation turn
        processing_time_partial = int((perf_counter() - start_time) * 1000)
        
        # ‚úÖ Enhanced: Extract plan state for PLAN intention
        extra_metadata = {"plan": agent_result.get("plan")}
        
        if intent_result.intention.value == "PLAN":
            from src.agents.context_builder import PlanContextExtractor
            # Extract plan parameters from query for future reference
            extracted_params = PlanContextExtractor.extract_from_query(
                request.query,
                request.language or "es"
            )
            if extracted_params:
                extra_metadata["plan_params"] = extracted_params
        
        await conversation_repo.save_turn(
            user_id=user_id,
            session_id=session_uuid,
            user_query=request.query,
            query_language=request.language or "es",
            intention=intent_result.intention.value,
            confidence=intent_result.confidence,
            complexity=intent_result.complexity,
            model_used=selected_model.name,
            model_provider=selected_model.provider,
            agent_response=response_text,
            places_found=agent_result.get("places"),
            processing_time_ms=processing_time_partial,
            tool_calls=agent_result.get("tool_calls", 0),
            reasoning_steps=agent_result.get("reasoning_steps", 0),
            extra_metadata=extra_metadata,
        )
        # Commit immediately so data is available for subsequent queries
        await conversation_repo.session.commit()
        
        # ‚úÖ NEW: Invalidate cache after saving new turn
        await memory_manager.invalidate_session_cache(session_uuid)
        
        # Step 8: Send final response
        processing_time = int((perf_counter() - start_time) * 1000)
        
        end_data = {
            "content": response_text,
            "places": agent_result.get("places", []),
            "plan": agent_result.get("plan"),
            "metadata": {
                "intention": intent_result.intention.value,
                "confidence": intent_result.confidence,
                "model_used": selected_model.name,
                "processing_time_ms": processing_time,
                "session_id": str(session_uuid),
            }
        }
        yield f"event: end\ndata: {json.dumps(end_data)}\n\n"
        
    except Exception as exc:
        logger.error("streaming_error", query_id=query_id, error=str(exc), exc_info=True)
        error_data = {"content": f"Error: {str(exc)}"}
        yield f"event: error\ndata: {json.dumps(error_data)}\n\n"


@router.post("/query/stream")
async def query_agent_stream(
    request: QueryRequest,
    validator: ContextValidator = Depends(get_context_validator),
    classifier: IntentClassifier = Depends(get_intent_classifier),
    llm_router: LLMRouter = Depends(get_llm_router),
    conversation_repo: ConversationRepository = Depends(get_conversation_repo),
    chat_repo: ChatRepository = Depends(get_chat_repo),
    memory_manager: MemoryManager = Depends(get_memory_manager),
    translator: Translator = Depends(get_translator_instance),
):
    """
    Stream agent response with real-time reasoning steps.
    
    Returns Server-Sent Events (SSE) with:
    - status: Processing updates
    - thought: Agent reasoning
    - action: Tools being used
    - observation: Tool results
    - token: Response text
    - end: Final response
    """
    return StreamingResponse(
        stream_agent_response(
            request,
            validator,
            classifier,
            llm_router,
            conversation_repo,
            chat_repo,
            memory_manager,
            translator,
        ),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        },
    )
